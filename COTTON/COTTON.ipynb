{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea04c4-92d9-4ed3-8567-3417c7a4c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras_tuner import HyperModel, RandomSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87dab0f-c1b8-4c98-965a-815958d61b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Total_Crops.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07682d09-d3bd-44c1-af1b-fc30469424ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291dc014-8fc8-4758-9748-375d4b20032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the dataset contains 'cropid' and 'mandiid' columns\n",
    "if 'cropid' in data.columns and 'mandiid' in data.columns:\n",
    "    # Get the count of entries for each combination of 'cropid' and 'mandiid'\n",
    "    combination_counts = data.groupby(['cropid', 'mandiid']).size().reset_index(name='count')\n",
    "\n",
    "    # Sort the combinations by count in descending order\n",
    "    combination_counts_sorted = combination_counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "    # Get the top 10 unique entries based on the counts\n",
    "    top_10_unique_combinations = combination_counts_sorted.head(10)\n",
    "\n",
    "    # Print the top 10 unique combinations without indexes\n",
    "    print(top_10_unique_combinations.to_string(index=False))\n",
    "else:\n",
    "    print(\"The dataset does not contain the required 'cropid' and 'mandiid' columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf7bd2-6f31-41e4-bf28-625f3e20188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for cropid and mandiid\n",
    "cropid = int(input(\"Enter cropid: \"))\n",
    "mandiid_input = input(\"Enter mandiid (leave blank for all mandis): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd20eac-e01c-4731-aff0-4c9fa37ace82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data based on inputs\n",
    "if mandiid_input.strip() == \"\":\n",
    "    filtered_data = data[data['cropid'] == cropid]\n",
    "else:\n",
    "    mandiid = int(mandiid_input)\n",
    "    filtered_data = data[(data['cropid'] == cropid) & (data['mandiid'] == mandiid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de2bfa-6f57-44af-8a4d-1c2936e74a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c6e66-b269-4c85-b2aa-f297090da25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers using Z-score method\n",
    "z_scores = np.abs(stats.zscore(filtered_data['modalprice']))\n",
    "filtered_data = filtered_data[z_scores < 3]\n",
    "print(f\"Data size after removing outliers: {filtered_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20b349-2e51-4ac2-a02e-97ed3c1d47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if filtered data is not empty\n",
    "if filtered_data.empty:\n",
    "    print(\"No data found for the given cropid and mandiid.\")\n",
    "else:\n",
    "    cropname = filtered_data['cropname'].iloc[0]\n",
    "    mandiname = filtered_data['mandiname'].iloc[0] if mandiid_input.strip() != \"\" else \"All Mandis\"\n",
    "\n",
    "    # Prepare data for training\n",
    "    dataset = filtered_data['modalprice'].values  # Assuming 'modalprice' is the target variable\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset = np.reshape(dataset, (-1, 1))\n",
    "\n",
    "    # Normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "    # Split into train and test sets (80% training, 20% testing)\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "    # Create a dataset function\n",
    "    def create_dataset(dataset, look_back=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset) - look_back - 1):\n",
    "            a = dataset[i:(i + look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    look_back = 5  # Increase look_back to consider more past days\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c086c-b06f-40cc-9d51-21d344beb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(1, look_back)))\n",
    "\n",
    "        # Tune the number of units in the LSTM layers\n",
    "        for i in range(hp.Int('num_layers', 1, 3)):\n",
    "            model.add(LSTM(units=hp.Int('units_' + str(i), min_value=50, max_value=200, step=50), return_sequences=(i < 2)))\n",
    "            model.add(LeakyReLU(negative_slope=0.01))\n",
    "            model.add(Dropout(rate=hp.Float('dropout_' + str(i), min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        # Tune the learning rate for the optimizer\n",
    "        model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "                      loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "# Instantiate the hypermodel\n",
    "hypermodel = MyHyperModel()\n",
    "\n",
    "# Instantiate the tuner and perform hyperparameter tuning\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='lstm_tuning'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "tuner.search(trainX, trainY, epochs=200, batch_size=64, validation_split=0.1, callbacks=[early_stopping], verbose=2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of layers is {best_hps.get('num_layers')}.\n",
    "The optimal number of units in each layer are:\n",
    "{[best_hps.get('units_' + str(i)) for i in range(best_hps.get('num_layers'))]}.\n",
    "The optimal dropout rates are:\n",
    "{[best_hps.get('dropout_' + str(i)) for i in range(best_hps.get('num_layers'))]}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(trainX, trainY, epochs=300, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96ebc1-ce0b-4482-a144-9f2bd241da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating predictions\n",
    "def predict_future(model, testX, days, look_back):\n",
    "    predictions = []\n",
    "    current_input = testX[-1]  # Start with the last input from the test set\n",
    "    current_input = np.reshape(current_input, (1, 1, look_back))\n",
    "    for _ in range(days):\n",
    "        prediction = model.predict(current_input)\n",
    "        predictions.append(prediction[0][0])\n",
    "        current_input = np.append(current_input[:, :, 1:], prediction).reshape((1, 1, look_back))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0ee6b-b054-44d5-ae81-7fef22de74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for number of days for prediction\n",
    "days = int(input(\"Enter number of days to predict: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f5472-57c1-4e2a-9593-7bc21363b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the specified number of days\n",
    "predictions = predict_future(model, testX, days, look_back)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "\n",
    "# Prepare data for visualization with fixed start date\n",
    "start_date = pd.to_datetime('2024-03-04')\n",
    "prediction_dates = pd.date_range(start=start_date, periods=days)\n",
    "prediction_series = pd.Series(predictions.flatten(), index=prediction_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907f877-bb7e-4b9c-a6c7-20808e550372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standalone prediction plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(prediction_series, label='Predicted Prices', linestyle='--', marker='o')\n",
    "for i, txt in enumerate(predictions):\n",
    "    plt.annotate(f'{txt[0]:.2f}', (prediction_series.index[i], prediction_series.values[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "plt.title(f'Standalone Prediction for {cropname} ({mandiname})\\nNumber of days: {days}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=30)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c586fa-6af9-41b6-ada8-41d953c395a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined plot of historical data and predictions\n",
    "historical_series = pd.Series(filtered_data['modalprice'].values, index=pd.to_datetime(filtered_data['date']))\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.scatter(historical_series.index, historical_series.values, label='Historical Prices')\n",
    "plt.plot(prediction_series, label='Predicted Prices', linestyle='', marker='^')\n",
    "plt.title(f'Historical and Predicted Prices for {cropname} ({mandiname})\\nNumber of days: {days}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b39edc-97fb-4c65-9c6a-ccfcd998f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to a DataFrame\n",
    "predicted_df = pd.DataFrame(predictions, columns=['predicted_price'])\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output_path = f'Predicted_{cropname}_Using_Cotton.csv'\n",
    "predicted_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
